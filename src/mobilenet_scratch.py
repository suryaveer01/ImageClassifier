# -*- coding: utf-8 -*-
"""CNNModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRhl6GnWdLuhDojQoBZG_X_dK8WDl1Z4
"""

import shutil
import numpy as np
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.utils.data as data_utils
from torchvision.models.mobilenetv2 import InvertedResidual, MobileNet_V2_Weights, MobileNetV2
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()



# Define transforms for the data
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

dtd_path = '.data\dtd\dtd\images'
checkpoint_path = 'checkpoints\model.pt'
best_model_path = 'checkpoints\savebestmodel.pt'

valid_loss_min = np.Inf

# Load the DTD dataset
# trainset = torchvision.datasets.DTD(root='.data',split='train',download=True, transform=train_transforms)
# trainset = torchvision.datasets.DTD(root='.data', split='train', download=False, transform=train_transforms)
dtd_dataset = torchvision.datasets.ImageFolder(root=dtd_path, transform=train_transforms)

# train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)
# testset = torchvision.datasets.DTD(root='.data',split='test',download=True, transform=test_transforms)
# test_loader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)

# Use random_split to split the dataset into train and test sets

train_size = int(0.8 * len(dtd_dataset))
test_size = len(dtd_dataset) - train_size
train_dataset, test_dataset = data_utils.random_split(dtd_dataset, [train_size, test_size])

# train_dataset = datasets.DTD(root="data",split='train',download=True, transform=transform)
# test_dataset = datasets.DTD(root="data",split = 'test',download = True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)


class MobileNet(nn.Module):
    def __init__(self, num_classes=47):
        super(MobileNet, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            self._make_layer(32, 64, stride=1),
            self._make_layer(64, 128, stride=2),
            self._make_layer(128, 128, stride=1),
            self._make_layer(128, 256, stride=2),
            
            self._make_layer(256, 256, stride=1),
            self._make_layer(256, 512, stride=2),
            
            # Repeat the following block 5 times
            # self._make_layer(512, 512, stride=1),
            # self._make_layer(512, 512, stride=1),
            # self._make_layer(512, 512, stride=1),
            # self._make_layer(512, 512, stride=1),
            # self._make_layer(512, 512, stride=1),
            
            # self._make_layer(512, 1024, stride=2),
            # self._make_layer(1024, 1024, stride=1),
            
            
            # self._make_layer(512, 256, stride=1),
            # self._make_layer(256, 128, stride=1),
            # self._make_layer(128, 64, stride=1),

            nn.AdaptiveAvgPool2d(1)
           
        )
        self.dropout = nn.Dropout(p=0.2, inplace=True)
        self.fc = nn.Linear(512, num_classes)
        self._initialize_weights()

    # def _make_layer(self, in_channels, out_channels, stride,groups):
    #     layers = []
    #     layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1,groups=groups, bias=False))
    #     layers.append(nn.BatchNorm2d(out_channels))
    #     layers.append(nn.ReLU(inplace=True))

    #     # layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))
    #     # layers.append(nn.BatchNorm2d(out_channels))
    #     # layers.append(nn.ReLU(inplace=True))
    #     # return nn.Sequential(*layers)
    def _make_layer(self, in_channels, out_channels, stride):
        layers = []
        # Depthwise convolution
        layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False))
        layers.append(nn.BatchNorm2d(in_channels))
        layers.append(nn.ReLU(inplace=True))
        # Pointwise convolution
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.model(x)
        x= self.dropout(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create an instance of MobileNet
model = MobileNet()
# Print the architecture
print(sum(p.numel() for p in model.parameters()))


# Define the optimizer and loss function
optimizer = optim.Adam(model.parameters(), lr=0.001)
# optimizer = optim.SGD(model.parameters(), lr=1e-03, momentum=0.9)
criterion = nn.CrossEntropyLoss()

#save checkpoiint
def save_ckp(state, is_best, checkpoint_path, best_model_path):
    """
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    """
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)


# Train the model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

model.to(device)

num_epochs = 100
for epoch in range(num_epochs):
    print(epoch)
    running_loss = 0.0
    running_corrects = 0

    running_train_loss = 0.0
    running_train_accuracy = 0.0

    model.train()
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        if device == 'cuda':
            inputs,lables = inputs.cuda(),labels.cuda()
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        _, predicted = outputs.max(1)
        correct = predicted.eq(labels).sum().item()
        accuracy = correct / labels.size(0)
        running_train_loss += loss.item()
        running_train_accuracy += accuracy
    
    # Calculate average training loss and accuracy for the epoch
    train_loss = running_train_loss / len(train_loader)
    train_accuracy = running_train_accuracy / len(train_loader)
    writer.add_scalar("Loss/train", train_loss, epoch)
    writer.add_scalar("accuracy/train", train_accuracy, epoch)
    print(' Epoch %d Training loss: %.3f acc: %.3f' % (epoch + 1, train_loss, train_accuracy))

    correct = 0
    total = 0
    model.eval()
    model.to(device)
    running_val_loss = 0.0
    running_val_accuracy = 0.0

    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            if device == 'cuda':
                inputs,lables = inputs.cuda(),labels.cuda()
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            # _, predicted = torch.max(outputs.data, 1)
            # total += labels.size(0)
            # correct += (predicted == labels).sum().item()
             # Update validation statistics
            _, predicted = outputs.max(1)
            correct = predicted.eq(labels).sum().item()
            accuracy = correct / labels.size(0)
            running_val_loss += loss.item()
            running_val_accuracy += accuracy
    
    # Calculate average validation loss and accuracy for the epoch
    val_loss = running_val_loss / len(test_loader)
    val_accuracy = running_val_accuracy / len(test_loader)
    writer.add_scalar("Loss/test", val_loss, epoch)
    writer.add_scalar("accuracy/test", val_accuracy, epoch)
    # print('Epoch %d Validation Accuracy of the network on the test images: %d %%' % (epoch+1,100 * correct / total))
    print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}')

    checkpoint = {
            'epoch': epoch + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
    
    save_ckp(checkpoint, False, checkpoint_path, best_model_path)
    
    ## TODO: save the model if validation loss has decreased
    if val_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
        # save checkpoint as best model
        save_ckp(checkpoint, True, checkpoint_path, best_model_path)
        valid_loss_min = val_loss

writer.flush()
print('Finished Training')
writer.close


# saving model

filename = 'model.pt'
torch.save(model.state_dict(), filename)